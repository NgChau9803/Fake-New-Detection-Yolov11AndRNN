# Dataset Configuration
data:
  fakeddit:
    files:
      - "data/raw/fakeddit/multimodal_train_sample.tsv"
      - "data/raw/fakeddit/multimodal_test_sample.tsv"
      - "data/raw/fakeddit/multimodal_eval_sample.tsv"
    file_type: "tsv"
    id_column: "id"
    text_column: "title"
    clean_text_column: "clean_title"
    image_url_column: "image_url"
    label_column: "2_way_label"
    metadata_columns:
      - "author"
      - "subreddit"
      - "domain"
      - "score"
      - "upvote_ratio"
  
  fakenewnet:
    dataset_dir: "data/raw/fakenewnet"  # Base directory for the dataset
    sources: ["politifact", "gossipcop"]  # Available sources
    labels: ["fake", "real"]  # Available labels
    id_column: "id"
    text_column: "text"
    clean_text_column: "clean_text"
    image_url_column: "image_url"
    label_column: "label"
    metadata_columns:
      - "source"
      - "publish_date"
      - "source_url"
      - "authors"
      - "keywords"
      - "canonical_link"
      - "summary"
  
  processed_dir: "data/processed"
  images_dir: "data/images"
  cache_dir: "data/cache"
  train_ratio: 0.8
  val_ratio: 0.1
  test_ratio: 0.1
  max_text_length: 100
  vocab_size: 10000
  balanced_sampling: true
  cache_features: true

# Model Configuration
model:
  text:
    embedding_dim: 300
    rnn_units: 128
    dropout: 0.2
    attention_heads: 4
    attention_dropout: 0.1
    visualize_attention: true
    bidirectional: true
    layer_norm: true
  
  image:
    input_shape: [224, 224, 3]
    backbone: "yolov11-tiny"
    backbone_config:
      initial_filters: 32
      block_filters: [64, 128, 256]
      num_blocks: [3, 4, 6]
      use_batch_norm: true
      activation: "leaky_relu"
      activation_alpha: 0.1
    feature_extraction:
      pooling: "global_average"
      feature_dim: 512
      dropout: 0.3
    attention:
      enabled: true
      attention_dim: 256
      num_heads: 4
      dropout: 0.1
    augmentation:
      enabled: true
      flip: true
      rotation: 0.1
      zoom: 0.1
      contrast: 0.1
      brightness: 0.1
      shift: 0.1
      noise: 0.05
      blur: 0.05
  
  fusion:
    hidden_dims: [512, 128]
    dropout: 0.3
    fusion_method: "attention"  # Options: concat, attention, bilinear
    attention_config:
      num_heads: 4
      attention_dim: 256
      dropout: 0.1
    layer_norm: true
    residual: true
    
# Training Configuration
training:
  batch_size: 32
  epochs: 10
  learning_rate: 0.001
  early_stopping_patience: 3
  use_class_weights: true
  optimizer: "adam"
  optimizer_config:
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-07
  lr_schedule:
    enabled: true
    decay_rate: 0.9
    decay_steps: 1000
    warmup_steps: 100
  gradient_clipping:
    enabled: true
    clip_norm: 1.0
  mixed_precision: true
  
# Evaluation Configuration
evaluation:
  metrics: ["accuracy", "precision", "recall", "f1", "auc"]
  num_explanation_samples: 5
  cross_dataset_validation: true
  confusion_matrix: true
  classification_report: true
  visualization:
    attention_maps: true
    feature_maps: true
    class_activation_maps: true
    token_importance: true
    save_format: "png"
    dpi: 300
    colormap: "viridis"
    overlay_alpha: 0.5 